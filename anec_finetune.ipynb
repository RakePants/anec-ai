{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RakePants/anec-ai/blob/main/anec_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO7MEGbb6mtB"
      },
      "source": [
        "# Task 3. Artem Eremkin\n",
        "\n",
        "Finetuning SberAI's RuGPTs in huggingface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing env"
      ],
      "metadata": {
        "id": "KBFIaQdwyxcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyhc5yrzR75j",
        "outputId": "24be3b1e-1395-4f81-c512-2c619a89cb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 52.8 MB/s eta 0:00:00\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 kB 26.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (1.26.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml): started\n",
            "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.29.0.dev0-py3-none-any.whl size=6928727 sha256=e51f8f7fbbbd83c7288c23c327e9557079e90f6380c604c4415bde5134a2c8e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x0q7w3og/wheels/1c/6e/db/b90d9f8554f165a9beb90b593fde94e9e60919270aed78efa0\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.29.0.dev0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'transformers'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "The dataset here is stored on Google Drive. Also available in the Github repository."
      ],
      "metadata": {
        "id": "EWf-Ngt3y0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRTiRkjlFAP9",
        "outputId": "04b3d765-27d4-49ee-930d-a7d3447b1beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os4vOL5LTOmk",
        "outputId": "433a781d-1d90-44d5-d44a-925a4392d87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1P6WSIeTdV5",
        "outputId": "9a687af7-34c9-47c0-88a4-5aa191244b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJZtWu8u6nwL"
      },
      "outputs": [],
      "source": [
        "!mkdir models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoyX62qN_38l"
      },
      "source": [
        "## Training \n",
        "The following code downloads the model and tokenizer from huggingface and finetunes model for generating jokes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD6jS49-TAbM",
        "outputId": "b0df28aa-9d98-46a7-a98a-f2b11a939145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-15 08:05:13--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27473 (27K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  26.83K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-04-15 08:05:13 (10.1 MB/s) - ‘run_clm.py’ saved [27473/27473]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SberAI's script for finetuning ruGPT-3"
      ],
      "metadata": {
        "id": "dXu7o1EqzGuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCIERP8AS1Dl",
        "outputId": "6a85d95a-99c9-4e25-bfdd-7858fb8c9470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-15 08:05:26.903653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/15/2023 08:05:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/15/2023 08:05:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/anec_model/runs/Apr15_08-05-33_2be11917a13e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/anec_model,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/anec_model,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/15/2023 08:05:39 - INFO - datasets.builder - Using custom data configuration default-cdd67d98585b42b5\n",
            "04/15/2023 08:05:39 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "04/15/2023 08:05:39 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 3572.66it/s]\n",
            "04/15/2023 08:05:39 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "04/15/2023 08:05:39 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "04/15/2023 08:05:40 - INFO - datasets.builder - Generating train split\n",
            "04/15/2023 08:05:41 - INFO - datasets.builder - Generating validation split\n",
            "04/15/2023 08:05:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 641.48it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 608/608 [00:00<00:00, 267kB/s]\n",
            "[INFO|configuration_utils.py:668] 2023-04-15 08:05:41,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-15 08:05:41,707 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:502] 2023-04-15 08:05:41,912 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:668] 2023-04-15 08:05:42,121 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-15 08:05:42,122 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.71M/1.71M [00:00<00:00, 6.82MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 1.27M/1.27M [00:00<00:00, 6.13MB/s]\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-15 08:05:44,241 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:668] 2023-04-15 08:05:44,241 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-15 08:05:44,242 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:668] 2023-04-15 08:05:44,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-15 08:05:44,356 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:280] 2023-04-15 08:05:44,452 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 551M/551M [00:05<00:00, 102MB/s]\n",
            "[INFO|modeling_utils.py:2534] 2023-04-15 08:05:50,421 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-04-15 08:05:50,748 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3190] 2023-04-15 08:05:52,334 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3198] 2023-04-15 08:05:52,335 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:2839] 2023-04-15 08:05:52,552 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on dataset:   0% 0/70214 [00:00<?, ? examples/s]04/15/2023 08:05:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6600715df5e1f074.arrow\n",
            "Running tokenizer on dataset:   0% 0/17507 [00:00<?, ? examples/s]04/15/2023 08:06:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-473f4a3b6bf0c253.arrow\n",
            "Grouping texts in chunks of 2048:   0% 0/70214 [00:00<?, ? examples/s]04/15/2023 08:06:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4e6af17fe29517b6.arrow\n",
            "Grouping texts in chunks of 2048:   0% 0/17507 [00:00<?, ? examples/s]04/15/2023 08:06:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-455dac177e33b0be.arrow\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 5.53MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1769] 2023-04-15 08:06:09,911 >> ***** Running training *****\n",
            "[INFO|trainer.py:1770] 2023-04-15 08:06:09,911 >>   Num examples = 1,137\n",
            "[INFO|trainer.py:1771] 2023-04-15 08:06:09,911 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1772] 2023-04-15 08:06:09,911 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1773] 2023-04-15 08:06:09,911 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1774] 2023-04-15 08:06:09,911 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1775] 2023-04-15 08:06:09,911 >>   Total optimization steps = 3,411\n",
            "[INFO|trainer.py:1776] 2023-04-15 08:06:09,911 >>   Number of trainable parameters = 125,231,616\n",
            "{'loss': 2.9298, 'learning_rate': 4.267077103488713e-05, 'epoch': 0.44}\n",
            " 15% 500/3411 [07:15<42:18,  1.15it/s][INFO|trainer.py:2868] 2023-04-15 08:13:24,930 >> Saving model checkpoint to models/anec_model/checkpoint-500\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:13:24,931 >> Configuration saved in models/anec_model/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:13:24,931 >> Configuration saved in models/anec_model/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:13:26,702 >> Model weights saved in models/anec_model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:13:26,703 >> tokenizer config file saved in models/anec_model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:13:26,703 >> Special tokens file saved in models/anec_model/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.8541, 'learning_rate': 3.5341542069774266e-05, 'epoch': 0.88}\n",
            " 29% 1000/3411 [14:37<35:15,  1.14it/s][INFO|trainer.py:2868] 2023-04-15 08:20:47,763 >> Saving model checkpoint to models/anec_model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:20:47,764 >> Configuration saved in models/anec_model/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:20:47,764 >> Configuration saved in models/anec_model/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:20:49,513 >> Model weights saved in models/anec_model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:20:49,514 >> tokenizer config file saved in models/anec_model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:20:49,514 >> Special tokens file saved in models/anec_model/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.6926, 'learning_rate': 2.801231310466139e-05, 'epoch': 1.32}\n",
            " 44% 1500/3411 [22:03<27:44,  1.15it/s][INFO|trainer.py:2868] 2023-04-15 08:28:13,695 >> Saving model checkpoint to models/anec_model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:28:13,696 >> Configuration saved in models/anec_model/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:28:13,696 >> Configuration saved in models/anec_model/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:28:15,404 >> Model weights saved in models/anec_model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:28:15,405 >> tokenizer config file saved in models/anec_model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:28:15,405 >> Special tokens file saved in models/anec_model/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.6476, 'learning_rate': 2.068308413954852e-05, 'epoch': 1.76}\n",
            " 59% 2000/3411 [29:26<20:33,  1.14it/s][INFO|trainer.py:2868] 2023-04-15 08:35:36,846 >> Saving model checkpoint to models/anec_model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:35:36,847 >> Configuration saved in models/anec_model/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:35:36,847 >> Configuration saved in models/anec_model/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:35:38,556 >> Model weights saved in models/anec_model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:35:38,557 >> tokenizer config file saved in models/anec_model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:35:38,557 >> Special tokens file saved in models/anec_model/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.5737, 'learning_rate': 1.335385517443565e-05, 'epoch': 2.2}\n",
            " 73% 2500/3411 [36:50<13:19,  1.14it/s][INFO|trainer.py:2868] 2023-04-15 08:43:00,648 >> Saving model checkpoint to models/anec_model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:43:00,650 >> Configuration saved in models/anec_model/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:43:00,651 >> Configuration saved in models/anec_model/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:43:02,406 >> Model weights saved in models/anec_model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:43:02,407 >> tokenizer config file saved in models/anec_model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:43:02,407 >> Special tokens file saved in models/anec_model/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.5114, 'learning_rate': 6.02462620932278e-06, 'epoch': 2.64}\n",
            " 88% 3000/3411 [44:13<05:58,  1.15it/s][INFO|trainer.py:2868] 2023-04-15 08:50:23,734 >> Saving model checkpoint to models/anec_model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:50:23,735 >> Configuration saved in models/anec_model/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:50:23,735 >> Configuration saved in models/anec_model/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:50:25,413 >> Model weights saved in models/anec_model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:50:25,414 >> tokenizer config file saved in models/anec_model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:50:25,415 >> Special tokens file saved in models/anec_model/checkpoint-3000/special_tokens_map.json\n",
            "100% 3411/3411 [50:18<00:00,  1.15it/s][INFO|trainer.py:2039] 2023-04-15 08:56:28,808 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3018.9215, 'train_samples_per_second': 1.13, 'train_steps_per_second': 1.13, 'train_loss': 2.679189234107025, 'epoch': 3.0}\n",
            "100% 3411/3411 [50:18<00:00,  1.13it/s]\n",
            "[INFO|trainer.py:2868] 2023-04-15 08:56:28,836 >> Saving model checkpoint to models/anec_model\n",
            "[INFO|configuration_utils.py:457] 2023-04-15 08:56:28,837 >> Configuration saved in models/anec_model/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-15 08:56:28,837 >> Configuration saved in models/anec_model/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-15 08:56:30,652 >> Model weights saved in models/anec_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-15 08:56:30,653 >> tokenizer config file saved in models/anec_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-15 08:56:30,653 >> Special tokens file saved in models/anec_model/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.6792\n",
            "  train_runtime            = 0:50:18.92\n",
            "  train_samples            =       1137\n",
            "  train_samples_per_second =       1.13\n",
            "  train_steps_per_second   =       1.13\n",
            "04/15/2023 08:56:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3129] 2023-04-15 08:56:30,864 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3131] 2023-04-15 08:56:30,864 >>   Num examples = 285\n",
            "[INFO|trainer.py:3134] 2023-04-15 08:56:30,864 >>   Batch size = 1\n",
            "100% 285/285 [01:27<00:00,  3.25it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.5032\n",
            "  eval_loss               =     2.6963\n",
            "  eval_runtime            = 0:01:28.00\n",
            "  eval_samples            =        285\n",
            "  eval_samples_per_second =      3.239\n",
            "  eval_steps_per_second   =      3.239\n",
            "  perplexity              =     14.824\n",
            "[INFO|modelcard.py:451] 2023-04-15 08:57:59,052 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.503154809348726}]}\n"
          ]
        }
      ],
      "source": [
        "# Trains for 3 epochs\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --train_file drive/MyDrive/anec/train_anec.txt \\\n",
        "    --validation_file drive/MyDrive/anec/valid_anec.txt \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --block_size 2048 \\\n",
        "    --dataset_config_name plain_text \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir models/anec_model --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "edoKxJbrANuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- **train metrics** --  \n",
        "epoch                    =        3.0  \n",
        "train_loss               =     2.6792  \n",
        "train_runtime            = 0:52:26.95  \n",
        "train_samples            =       1137  \n",
        "tran_samples_per_second =      1.084  \n",
        "train_steps_per_second   =      1.084  \n",
        "\n",
        "-- **eval metrics** --   \n",
        "  epoch                   =        3.0  \n",
        "  eval_accuracy           =     0.5032  \n",
        "  eval_loss               =     2.6963  \n",
        "  eval_runtime            = 0:01:32.85  \n",
        "  eval_samples            =        285  \n",
        "  eval_samples_per_second =      3.069  \n",
        "  eval_steps_per_second   =      3.069  \n",
        "  perplexity              =     14.824  "
      ],
      "metadata": {
        "id": "0xltG6v8--Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity is 14.824, while for the original model it lays in [13, 15]"
      ],
      "metadata": {
        "id": "N8f4OOPJATwv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvgntLymArg3"
      },
      "source": [
        "## Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csHcDJXFDdaW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting random seed"
      ],
      "metadata": {
        "id": "BkJBWslmzYic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJxPg-cJDhAB",
        "outputId": "5c8c0cb2-824c-437a-b6e1-6aa65ec6b641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f711c15b350>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkUrzKsy_16F"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_EMbgO0BTvb"
      },
      "outputs": [],
      "source": [
        "tok = GPT2Tokenizer.from_pretrained(\"models/anec_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjy0GAuQBYpA"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"models/anec_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irh4H-HDBb6V",
        "outputId": "1bdacc8d-9e13-45dd-911a-cc1b2999b903",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50264, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Placing the model to CUDA\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating answer"
      ],
      "metadata": {
        "id": "xf4fLTPdzwld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gfJFmeOBj_t",
        "outputId": "f8347944-29b0-4d1e-d254-ac02a3166e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "prompts = ['Пошел мужик в лес', 'В газете написали, что', 'Васька сказал сегодня', 'Посмотри на это', \n",
        "           'Кто же мог подумать', 'Заходят в бар три', 'Про тебя рассказывают, что ты', 'Как-то раз',\n",
        "           'Летит мужик в самолете. И тут вдруг', 'Попал мужик в рай. Подходит к воротам'] \n",
        "rows = []\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    # Inference with different temperature for each prompt\n",
        "    inpt = tok.encode(prompt, return_tensors='pt')\n",
        "    out_07 = model.generate(inpt.cuda(), \n",
        "                        max_length=60, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=0.7)\n",
        "    \n",
        "    out_1 = model.generate(inpt.cuda(), \n",
        "                        max_length=60, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=1)\n",
        "    \n",
        "    out_13 = model.generate(inpt.cuda(), \n",
        "                        max_length=60, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=1.3)\n",
        "\n",
        "    # Filling the table\n",
        "    # splitting by '\\n' and taking the first line to get 1 joke\n",
        "    rows.append([prompt, \n",
        "             tok.decode(out_07[0]).replace('<s>', '').replace('</s>', '').split('\\n')[0],\n",
        "             tok.decode(out_1[0]).replace('<s>', '').replace('</s>', '').split('\\n')[0],\n",
        "             tok.decode(out_13[0]).replace('<s>', '').replace('</s>', '').split('\\n')[0]])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['Prompt', 'temp=0.7', 'temp=1.0', 'temp=1.3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "3XTm0hX1z4Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GA-erwbvDy5N",
        "outputId": "85a18b94-8e99-4c35-eb1c-309fdd46ab27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Prompt  \\\n",
              "0                      Пошел мужик в лес   \n",
              "1                 В газете написали, что   \n",
              "2                  Васька сказал сегодня   \n",
              "3                        Посмотри на это   \n",
              "4                    Кто же мог подумать   \n",
              "5                      Заходят в бар три   \n",
              "6          Про тебя рассказывают, что ты   \n",
              "7                             Как-то раз   \n",
              "8    Летит мужик в самолете. И тут вдруг   \n",
              "9  Попал мужик в рай. Подходит к воротам   \n",
              "\n",
              "                                            temp=0.7  \\\n",
              "0                      Пошел мужик в лес за грибами.   \n",
              "1  В газете написали, что в стране кризис, а Пути...   \n",
              "2  Васька сказал сегодня, что я - говно. И что? О...   \n",
              "3  Посмотри на это! В наше время это называется \"...   \n",
              "4   Кто же мог подумать, что я буду ходить по земле!   \n",
              "5  Заходят в бар три пьяных мужика и начинают пет...   \n",
              "6  Про тебя рассказывают, что ты - не такая. Не т...   \n",
              "7  Как-то раз я поймал щуку и, как всегда, поймал...   \n",
              "8  Летит мужик в самолете. И тут вдруг: - А почем...   \n",
              "9  Попал мужик в рай. Подходит к воротам, а там -...   \n",
              "\n",
              "                                            temp=1.0  \\\n",
              "0  Пошел мужик в лес и нашел в лесу скелет. Как, ...   \n",
              "1  В газете написали, что у вас в квартире кто-то...   \n",
              "2  Васька сказал сегодня, что я ему нравлюсь. Я е...   \n",
              "3               Посмотри на это... Это же \"Кокетка\"!   \n",
              "4  Кто же мог подумать, что я возьму в жены столь...   \n",
              "5  Заходят в бар три пьяных еврея и требуют, чтоб...   \n",
              "6  Про тебя рассказывают, что ты - \"самый красивы...   \n",
              "7  Как-то раз я случайно увидел ее во сне. А я ни...   \n",
              "8  Летит мужик в самолете. И тут вдруг, откуда не...   \n",
              "9  Попал мужик в рай. Подходит к воротам, там муж...   \n",
              "\n",
              "                                            temp=1.3  \n",
              "0  Пошел мужик в лес за грибами, грибы нашел и......  \n",
              "1  В газете написали, что на улице было 20 тысяч ...  \n",
              "2  Васька сказал сегодня на работе... Я ему: \"Вас...  \n",
              "3  Посмотри на это платье! Ты же просила, чтобы е...  \n",
              "4  Кто же мог подумать, что ты будешь любить ее?!...  \n",
              "5      Заходят в бар три парня в очках и пьют водку.  \n",
              "6  Про тебя рассказывают, что ты стал очень хорош...  \n",
              "7  Как-то раз ночью, когда я был маленьким, я вид...  \n",
              "8             Летит мужик в самолете. И тут вдруг...  \n",
              "9  Попал мужик в рай. Подходит к воротам... А ну ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c99e3938-ce39-4fb1-bc17-4c45153c50da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prompt</th>\n",
              "      <th>temp=0.7</th>\n",
              "      <th>temp=1.0</th>\n",
              "      <th>temp=1.3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Пошел мужик в лес</td>\n",
              "      <td>Пошел мужик в лес за грибами.</td>\n",
              "      <td>Пошел мужик в лес и нашел в лесу скелет. Как, ...</td>\n",
              "      <td>Пошел мужик в лес за грибами, грибы нашел и......</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>В газете написали, что</td>\n",
              "      <td>В газете написали, что в стране кризис, а Пути...</td>\n",
              "      <td>В газете написали, что у вас в квартире кто-то...</td>\n",
              "      <td>В газете написали, что на улице было 20 тысяч ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Васька сказал сегодня</td>\n",
              "      <td>Васька сказал сегодня, что я - говно. И что? О...</td>\n",
              "      <td>Васька сказал сегодня, что я ему нравлюсь. Я е...</td>\n",
              "      <td>Васька сказал сегодня на работе... Я ему: \"Вас...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Посмотри на это</td>\n",
              "      <td>Посмотри на это! В наше время это называется \"...</td>\n",
              "      <td>Посмотри на это... Это же \"Кокетка\"!</td>\n",
              "      <td>Посмотри на это платье! Ты же просила, чтобы е...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кто же мог подумать</td>\n",
              "      <td>Кто же мог подумать, что я буду ходить по земле!</td>\n",
              "      <td>Кто же мог подумать, что я возьму в жены столь...</td>\n",
              "      <td>Кто же мог подумать, что ты будешь любить ее?!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Заходят в бар три</td>\n",
              "      <td>Заходят в бар три пьяных мужика и начинают пет...</td>\n",
              "      <td>Заходят в бар три пьяных еврея и требуют, чтоб...</td>\n",
              "      <td>Заходят в бар три парня в очках и пьют водку.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Про тебя рассказывают, что ты</td>\n",
              "      <td>Про тебя рассказывают, что ты - не такая. Не т...</td>\n",
              "      <td>Про тебя рассказывают, что ты - \"самый красивы...</td>\n",
              "      <td>Про тебя рассказывают, что ты стал очень хорош...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Как-то раз</td>\n",
              "      <td>Как-то раз я поймал щуку и, как всегда, поймал...</td>\n",
              "      <td>Как-то раз я случайно увидел ее во сне. А я ни...</td>\n",
              "      <td>Как-то раз ночью, когда я был маленьким, я вид...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Летит мужик в самолете. И тут вдруг</td>\n",
              "      <td>Летит мужик в самолете. И тут вдруг: - А почем...</td>\n",
              "      <td>Летит мужик в самолете. И тут вдруг, откуда не...</td>\n",
              "      <td>Летит мужик в самолете. И тут вдруг...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Попал мужик в рай. Подходит к воротам</td>\n",
              "      <td>Попал мужик в рай. Подходит к воротам, а там -...</td>\n",
              "      <td>Попал мужик в рай. Подходит к воротам, там муж...</td>\n",
              "      <td>Попал мужик в рай. Подходит к воротам... А ну ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c99e3938-ce39-4fb1-bc17-4c45153c50da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c99e3938-ce39-4fb1-bc17-4c45153c50da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c99e3938-ce39-4fb1-bc17-4c45153c50da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.to_markdown())"
      ],
      "metadata": {
        "id": "S365b-6CK8ax",
        "outputId": "6fa0457e-e816-4c12-d90f-200e5234ebdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | Prompt                                | temp=0.7                                                                                                                                                                                    | temp=1.0                                                                                                                                                                                                                       | temp=1.3                                                                                                                                                                                                                                                               |\n",
            "|---:|:--------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "|  0 | Пошел мужик в лес                     | Пошел мужик в лес за грибами.                                                                                                                                                               | Пошел мужик в лес и нашел в лесу скелет. Как, скелет, еще и в воду ходил? Да, мужик. И нашел его скелет. Как, скелет, еще и в воду ходил? Да, мужик. И нашел скелет. Как, скелет, еще и в воду ходил? Да,                      | Пошел мужик в лес за грибами, грибы нашел и... Ну и что? А то, как он все время грибы искал - так что грибы и не нашли.                                                                                                                                                |\n",
            "|  1 | В газете написали, что                | В газете написали, что в стране кризис, а Путин в Кремле. Так он же не дурак!                                                                                                               | В газете написали, что у вас в квартире кто-то живет. Как? Ну живут же люди!!!                                                                                                                                                 | В газете написали, что на улице было 20 тысяч людей, не смотря на холод и сильный ветер? Ну-у, может быть и 20 тысяч. Что вы хотели: 20 тысяч людей - и на улице! В газете так же написано, что они были и на другой улице? Так вы утверждаете                         |\n",
            "|  2 | Васька сказал сегодня                 | Васька сказал сегодня, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, | Васька сказал сегодня, что я ему нравлюсь. Я его дураком сделал. Я его проституткой сделал. Я его шлюхой сделал. Я его проституткой сделал. Я его шлюхой сделал. Я его шлюхой сделал. Ты ему в рот не клади. Это что, его жена | Васька сказал сегодня на работе... Я ему: \"Вася, ну что же у вас в голове происходит? Да вот в голову залетела птица!!! \". А Васька? Он: \"Уйди с работы, я больше никогда не допущу! \"                                                                                 |\n",
            "|  3 | Посмотри на это                       | Посмотри на это! В наше время это называется \"пьяный король\"!                                                                                                                               | Посмотри на это... Это же \"Кокетка\"!                                                                                                                                                                                           | Посмотри на это платье! Ты же просила, чтобы его украшали бриллиантовые подвески! Ну да! Вот, теперь и серьги будут!                                                                                                                                                   |\n",
            "|  4 | Кто же мог подумать                   | Кто же мог подумать, что я буду ходить по земле!                                                                                                                                            | Кто же мог подумать, что я возьму в жены столь умного и красивого мужчину? Ты просто не знаешь, что у меня есть жена. Да, конечно, ведь я же не женат.                                                                         | Кто же мог подумать, что ты будешь любить ее?! Она ведь моя тетя, а я не хочу разбрасывать в людях соль.                                                                                                                                                               |\n",
            "|  5 | Заходят в бар три                     | Заходят в бар три пьяных мужика и начинают петь. А ты что? Я же их всех в рот брал!                                                                                                         | Заходят в бар три пьяных еврея и требуют, чтобы им дали водки.                                                                                                                                                                 | Заходят в бар три парня в очках и пьют водку.                                                                                                                                                                                                                          |\n",
            "|  6 | Про тебя рассказывают, что ты         | Про тебя рассказывают, что ты - не такая. Не такая, как все. Не такая, как все.                                                                                                             | Про тебя рассказывают, что ты - \"самый красивый мужчина на свете\". Да, и ты даже больше, чем все, кого я знаю.                                                                                                                 | Про тебя рассказывают, что ты стал очень хорошим другом! Я не о тебе. Я об Ане. Мы как два разных человека...                                                                                                                                                          |\n",
            "|  7 | Как-то раз                            | Как-то раз я поймал щуку и, как всегда, поймал ее. А я не поймал, я ее поймал!                                                                                                              | Как-то раз я случайно увидел ее во сне. А я никогда не видел ее во сне.                                                                                                                                                        | Как-то раз ночью, когда я был маленьким, я видел, как по шоссе движется машина с надписью «Рено». В чем дело, ты не запомнил? Машина ехала со скоростью восемьдесят километров в час! Но почему же так рано? Это просто потому, что в те времена дороги были такими же |\n",
            "|  8 | Летит мужик в самолете. И тут вдруг   | Летит мужик в самолете. И тут вдруг: - А почему он не кричит? Это я его в туалете из туалета вытащил.                                                                                       | Летит мужик в самолете. И тут вдруг, откуда не возьмись, на него на бреющем полете летят два самолета. И летчик падает! А летчики? А летчики его ловят...                                                                      | Летит мужик в самолете. И тут вдруг...                                                                                                                                                                                                                                 |\n",
            "|  9 | Попал мужик в рай. Подходит к воротам | Попал мужик в рай. Подходит к воротам, а там - бабах!                                                                                                                                       | Попал мужик в рай. Подходит к воротам, там мужик в футболке с надписью \"ОБЛАДАНИЕ В ПОДАРОК\". - И что там? Дураки и дороги.                                                                                                    | Попал мужик в рай. Подходит к воротам... А ну открой!                                                                                                                                                                                                                  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|    | Prompt                                | temp=0.7                                                                                                                                                                                    | temp=1.0                                                                                                                                                                                                                       | temp=1.3                                                                                                                                                                                                                                                               |\n",
        "|---:|:--------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "|  0 | Пошел мужик в лес                     | Пошел мужик в лес за грибами.                                                                                                                                                               | Пошел мужик в лес и нашел в лесу скелет. Как, скелет, еще и в воду ходил? Да, мужик. И нашел его скелет. Как, скелет, еще и в воду ходил? Да, мужик. И нашел скелет. Как, скелет, еще и в воду ходил? Да,                      | Пошел мужик в лес за грибами, грибы нашел и... Ну и что? А то, как он все время грибы искал - так что грибы и не нашли.                                                                                                                                                |\n",
        "|  1 | В газете написали, что                | В газете написали, что в стране кризис, а Путин в Кремле. Так он же не дурак!                                                                                                               | В газете написали, что у вас в квартире кто-то живет. Как? Ну живут же люди!!!                                                                                                                                                 | В газете написали, что на улице было 20 тысяч людей, не смотря на холод и сильный ветер? Ну-у, может быть и 20 тысяч. Что вы хотели: 20 тысяч людей - и на улице! В газете так же написано, что они были и на другой улице? Так вы утверждаете                         |\n",
        "|  2 | Васька сказал сегодня                 | Васька сказал сегодня, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, что я - говно. И что? Он сказал, | Васька сказал сегодня, что я ему нравлюсь. Я его дураком сделал. Я его проституткой сделал. Я его шлюхой сделал. Я его проституткой сделал. Я его шлюхой сделал. Я его шлюхой сделал. Ты ему в рот не клади. Это что, его жена | Васька сказал сегодня на работе... Я ему: \"Вася, ну что же у вас в голове происходит? Да вот в голову залетела птица!!! \". А Васька? Он: \"Уйди с работы, я больше никогда не допущу! \"                                                                                 |\n",
        "|  3 | Посмотри на это                       | Посмотри на это! В наше время это называется \"пьяный король\"!                                                                                                                               | Посмотри на это... Это же \"Кокетка\"!                                                                                                                                                                                           | Посмотри на это платье! Ты же просила, чтобы его украшали бриллиантовые подвески! Ну да! Вот, теперь и серьги будут!                                                                                                                                                   |\n",
        "|  4 | Кто же мог подумать                   | Кто же мог подумать, что я буду ходить по земле!                                                                                                                                            | Кто же мог подумать, что я возьму в жены столь умного и красивого мужчину? Ты просто не знаешь, что у меня есть жена. Да, конечно, ведь я же не женат.                                                                         | Кто же мог подумать, что ты будешь любить ее?! Она ведь моя тетя, а я не хочу разбрасывать в людях соль.                                                                                                                                                               |\n",
        "|  5 | Заходят в бар три                     | Заходят в бар три пьяных мужика и начинают петь. А ты что? Я же их всех в рот брал!                                                                                                         | Заходят в бар три пьяных еврея и требуют, чтобы им дали водки.                                                                                                                                                                 | Заходят в бар три парня в очках и пьют водку.                                                                                                                                                                                                                          |\n",
        "|  6 | Про тебя рассказывают, что ты         | Про тебя рассказывают, что ты - не такая. Не такая, как все. Не такая, как все.                                                                                                             | Про тебя рассказывают, что ты - \"самый красивый мужчина на свете\". Да, и ты даже больше, чем все, кого я знаю.                                                                                                                 | Про тебя рассказывают, что ты стал очень хорошим другом! Я не о тебе. Я об Ане. Мы как два разных человека...                                                                                                                                                          |\n",
        "|  7 | Как-то раз                            | Как-то раз я поймал щуку и, как всегда, поймал ее. А я не поймал, я ее поймал!                                                                                                              | Как-то раз я случайно увидел ее во сне. А я никогда не видел ее во сне.                                                                                                                                                        | Как-то раз ночью, когда я был маленьким, я видел, как по шоссе движется машина с надписью «Рено». В чем дело, ты не запомнил? Машина ехала со скоростью восемьдесят километров в час! Но почему же так рано? Это просто потому, что в те времена дороги были такими же |\n",
        "|  8 | Летит мужик в самолете. И тут вдруг   | Летит мужик в самолете. И тут вдруг: - А почему он не кричит? Это я его в туалете из туалета вытащил.                                                                                       | Летит мужик в самолете. И тут вдруг, откуда не возьмись, на него на бреющем полете летят два самолета. И летчик падает! А летчики? А летчики его ловят...                                                                      | Летит мужик в самолете. И тут вдруг...                                                                                                                                                                                                                                 |\n",
        "|  9 | Попал мужик в рай. Подходит к воротам | Попал мужик в рай. Подходит к воротам, а там - бабах!                                                                                                                                       | Попал мужик в рай. Подходит к воротам, там мужик в футболке с надписью \"ОБЛАДАНИЕ В ПОДАРОК\". - И что там? Дураки и дороги.                                                                                                    | Попал мужик в рай. Подходит к воротам... А ну открой!                                                                                                                                                                                                                  |"
      ],
      "metadata": {
        "id": "QmfS3mqLlY55"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}