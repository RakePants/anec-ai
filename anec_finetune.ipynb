{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO7MEGbb6mtB"
      },
      "source": [
        "# Task 3. Text Generation\n",
        "\n",
        "Finetuning SberAI's RuGPTs in huggingface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing env"
      ],
      "metadata": {
        "id": "KBFIaQdwyxcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xyhc5yrzR75j",
        "outputId": "73a13203-657c-4db7-b835-21ee03c906d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 kB 6.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (3.11.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 76.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml): started\n",
            "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.29.0.dev0-py3-none-any.whl size=6928680 sha256=46a6c42087833cb0c5ba9fab438a8851f1846425475ef01049f4381759358e83\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2w_3qrqd/wheels/1c/6e/db/b90d9f8554f165a9beb90b593fde94e9e60919270aed78efa0\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.29.0.dev0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'transformers'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "The dataset is stored on Google Drive"
      ],
      "metadata": {
        "id": "EWf-Ngt3y0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRTiRkjlFAP9",
        "outputId": "99c69410-889c-484b-f780-188dddc4df4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Os4vOL5LTOmk",
        "outputId": "5c7914ca-3a18-4552-ab52-9a334ed5d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m1P6WSIeTdV5",
        "outputId": "a8839459-e6d8-4c63-f904-cb4f4c797253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.11.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WJZtWu8u6nwL"
      },
      "outputs": [],
      "source": [
        "!mkdir models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoyX62qN_38l"
      },
      "source": [
        "## Training \n",
        "The following code downloads the model and tokenizer from huggingface and finetunes model for generating jokes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SD6jS49-TAbM",
        "outputId": "2bd92408-fa4f-4121-e6f7-5573d99ed783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-14 17:21:50--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27473 (27K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  26.83K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-04-14 17:21:50 (14.3 MB/s) - ‘run_clm.py’ saved [27473/27473]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SberAI's script for finetuning ruGPT-3"
      ],
      "metadata": {
        "id": "dXu7o1EqzGuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OCIERP8AS1Dl",
        "outputId": "c771b499-6294-4809-ac5a-42ae9a5fd51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-14 17:22:16.568747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/14/2023 17:22:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/14/2023 17:22:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/anec_model/runs/Apr14_17-22-19_dccabbaecba2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/anec_model,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/anec_model,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2023 17:22:24 - INFO - datasets.builder - Using custom data configuration default-cdd67d98585b42b5\n",
            "04/14/2023 17:22:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "04/14/2023 17:22:24 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 657.78it/s]\n",
            "04/14/2023 17:22:24 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "04/14/2023 17:22:24 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:01<00:00,  1.25it/s]\n",
            "04/14/2023 17:22:26 - INFO - datasets.builder - Generating train split\n",
            "04/14/2023 17:22:27 - INFO - datasets.builder - Generating validation split\n",
            "04/14/2023 17:22:28 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 448.47it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 608/608 [00:00<00:00, 207kB/s]\n",
            "[INFO|configuration_utils.py:668] 2023-04-14 17:22:28,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-14 17:22:28,514 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:502] 2023-04-14 17:22:28,700 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:668] 2023-04-14 17:22:28,881 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-14 17:22:28,882 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.71M/1.71M [00:00<00:00, 23.1MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 1.27M/1.27M [00:00<00:00, 19.3MB/s]\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,520 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,520 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,520 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,520 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,522 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1809] 2023-04-14 17:22:30,522 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:668] 2023-04-14 17:22:30,522 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-14 17:22:30,526 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:668] 2023-04-14 17:22:30,793 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-14 17:22:30,795 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.29.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:280] 2023-04-14 17:22:31,038 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 551M/551M [00:04<00:00, 134MB/s]\n",
            "[INFO|modeling_utils.py:2534] 2023-04-14 17:22:35,479 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/d64244b316057f71e745cc92be1dcfe7853d9d18/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-04-14 17:22:36,175 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3190] 2023-04-14 17:22:41,103 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3198] 2023-04-14 17:22:41,103 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:2839] 2023-04-14 17:22:41,352 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on dataset:   0% 0/70214 [00:00<?, ? examples/s]04/14/2023 17:22:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6600715df5e1f074.arrow\n",
            "Running tokenizer on dataset:   0% 0/17507 [00:00<?, ? examples/s]04/14/2023 17:22:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-473f4a3b6bf0c253.arrow\n",
            "Grouping texts in chunks of 2048:   0% 0/70214 [00:00<?, ? examples/s]04/14/2023 17:22:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4e6af17fe29517b6.arrow\n",
            "Grouping texts in chunks of 2048:   0% 0/17507 [00:00<?, ? examples/s]04/14/2023 17:23:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-cdd67d98585b42b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-455dac177e33b0be.arrow\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 5.49MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1769] 2023-04-14 17:23:06,337 >> ***** Running training *****\n",
            "[INFO|trainer.py:1770] 2023-04-14 17:23:06,337 >>   Num examples = 1,137\n",
            "[INFO|trainer.py:1771] 2023-04-14 17:23:06,337 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1772] 2023-04-14 17:23:06,337 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1773] 2023-04-14 17:23:06,337 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1774] 2023-04-14 17:23:06,337 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1775] 2023-04-14 17:23:06,337 >>   Total optimization steps = 3,411\n",
            "[INFO|trainer.py:1776] 2023-04-14 17:23:06,338 >>   Number of trainable parameters = 125,231,616\n",
            "{'loss': 2.9298, 'learning_rate': 4.267077103488713e-05, 'epoch': 0.44}\n",
            " 15% 500/3411 [07:38<44:03,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 17:30:44,392 >> Saving model checkpoint to models/anec_model/checkpoint-500\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 17:30:44,393 >> Configuration saved in models/anec_model/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 17:30:44,393 >> Configuration saved in models/anec_model/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 17:30:46,154 >> Model weights saved in models/anec_model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 17:30:46,155 >> tokenizer config file saved in models/anec_model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 17:30:46,155 >> Special tokens file saved in models/anec_model/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.8541, 'learning_rate': 3.5341542069774266e-05, 'epoch': 0.88}\n",
            " 29% 1000/3411 [15:20<36:35,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 17:38:27,216 >> Saving model checkpoint to models/anec_model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 17:38:27,217 >> Configuration saved in models/anec_model/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 17:38:27,218 >> Configuration saved in models/anec_model/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 17:38:29,033 >> Model weights saved in models/anec_model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 17:38:29,034 >> tokenizer config file saved in models/anec_model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 17:38:29,034 >> Special tokens file saved in models/anec_model/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.6926, 'learning_rate': 2.801231310466139e-05, 'epoch': 1.32}\n",
            " 44% 1500/3411 [23:02<29:01,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 17:46:08,501 >> Saving model checkpoint to models/anec_model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 17:46:08,502 >> Configuration saved in models/anec_model/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 17:46:08,502 >> Configuration saved in models/anec_model/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 17:46:10,320 >> Model weights saved in models/anec_model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 17:46:10,320 >> tokenizer config file saved in models/anec_model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 17:46:10,321 >> Special tokens file saved in models/anec_model/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.6476, 'learning_rate': 2.068308413954852e-05, 'epoch': 1.76}\n",
            " 59% 2000/3411 [30:43<21:22,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 17:53:49,913 >> Saving model checkpoint to models/anec_model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 17:53:49,914 >> Configuration saved in models/anec_model/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 17:53:49,914 >> Configuration saved in models/anec_model/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 17:53:51,625 >> Model weights saved in models/anec_model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 17:53:51,631 >> tokenizer config file saved in models/anec_model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 17:53:51,631 >> Special tokens file saved in models/anec_model/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.5737, 'learning_rate': 1.335385517443565e-05, 'epoch': 2.2}\n",
            " 73% 2500/3411 [38:25<13:50,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 18:01:31,600 >> Saving model checkpoint to models/anec_model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 18:01:31,601 >> Configuration saved in models/anec_model/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 18:01:31,602 >> Configuration saved in models/anec_model/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 18:01:33,297 >> Model weights saved in models/anec_model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 18:01:33,298 >> tokenizer config file saved in models/anec_model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 18:01:33,298 >> Special tokens file saved in models/anec_model/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.5114, 'learning_rate': 6.02462620932278e-06, 'epoch': 2.64}\n",
            " 88% 3000/3411 [46:06<06:14,  1.10it/s][INFO|trainer.py:2868] 2023-04-14 18:09:12,912 >> Saving model checkpoint to models/anec_model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 18:09:12,913 >> Configuration saved in models/anec_model/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 18:09:12,914 >> Configuration saved in models/anec_model/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 18:09:14,629 >> Model weights saved in models/anec_model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 18:09:14,630 >> tokenizer config file saved in models/anec_model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 18:09:14,630 >> Special tokens file saved in models/anec_model/checkpoint-3000/special_tokens_map.json\n",
            "100% 3411/3411 [52:26<00:00,  1.10it/s][INFO|trainer.py:2039] 2023-04-14 18:15:33,272 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3146.9595, 'train_samples_per_second': 1.084, 'train_steps_per_second': 1.084, 'train_loss': 2.679189234107025, 'epoch': 3.0}\n",
            "100% 3411/3411 [52:26<00:00,  1.08it/s]\n",
            "[INFO|trainer.py:2868] 2023-04-14 18:15:33,300 >> Saving model checkpoint to models/anec_model\n",
            "[INFO|configuration_utils.py:457] 2023-04-14 18:15:33,301 >> Configuration saved in models/anec_model/config.json\n",
            "[INFO|configuration_utils.py:362] 2023-04-14 18:15:33,301 >> Configuration saved in models/anec_model/generation_config.json\n",
            "[INFO|modeling_utils.py:1847] 2023-04-14 18:15:34,967 >> Model weights saved in models/anec_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2171] 2023-04-14 18:15:34,968 >> tokenizer config file saved in models/anec_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2178] 2023-04-14 18:15:34,968 >> Special tokens file saved in models/anec_model/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.6792\n",
            "  train_runtime            = 0:52:26.95\n",
            "  train_samples            =       1137\n",
            "  train_samples_per_second =      1.084\n",
            "  train_steps_per_second   =      1.084\n",
            "04/14/2023 18:15:35 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3129] 2023-04-14 18:15:35,215 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3131] 2023-04-14 18:15:35,215 >>   Num examples = 285\n",
            "[INFO|trainer.py:3134] 2023-04-14 18:15:35,215 >>   Batch size = 1\n",
            "100% 285/285 [01:32<00:00,  3.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.5032\n",
            "  eval_loss               =     2.6963\n",
            "  eval_runtime            = 0:01:32.85\n",
            "  eval_samples            =        285\n",
            "  eval_samples_per_second =      3.069\n",
            "  eval_steps_per_second   =      3.069\n",
            "  perplexity              =     14.824\n",
            "[INFO|modelcard.py:451] 2023-04-14 18:17:08,238 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.503154809348726}]}\n"
          ]
        }
      ],
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --train_file drive/MyDrive/anec/train_anec.txt \\\n",
        "    --validation_file drive/MyDrive/anec/valid_anec.txt \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --block_size 2048 \\\n",
        "    --dataset_config_name plain_text \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir models/anec_model --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics:\n",
        "\n",
        "-- **train metrics** --\n",
        "epoch                    =        3.0  \n",
        "train_loss               =     2.6792  \n",
        "train_runtime            = 0:52:26.95  \n",
        "train_samples            =       1137  \n",
        "tran_samples_per_second =      1.084  \n",
        "train_steps_per_second   =      1.084  \n",
        "\n",
        "-- **eval metrics** -- \n",
        "  epoch                   =        3.0  \n",
        "  eval_accuracy           =     0.5032  \n",
        "  eval_loss               =     2.6963  \n",
        "  eval_runtime            = 0:01:32.85  \n",
        "  eval_samples            =        285  \n",
        "  eval_samples_per_second =      3.069  \n",
        "  eval_steps_per_second   =      3.069  \n",
        "  perplexity              =     14.824  "
      ],
      "metadata": {
        "id": "0xltG6v8--Ml"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvgntLymArg3"
      },
      "source": [
        "## Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "csHcDJXFDdaW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting random seed"
      ],
      "metadata": {
        "id": "BkJBWslmzYic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TJxPg-cJDhAB",
        "outputId": "b83ef5e1-7c64-48c0-fbc2-4503f08f9516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4397e0e450>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AkUrzKsy_16F"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x_EMbgO0BTvb"
      },
      "outputs": [],
      "source": [
        "tok = GPT2Tokenizer.from_pretrained(\"models/anec_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fjy0GAuQBYpA"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"models/anec_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "irh4H-HDBb6V",
        "outputId": "cb143722-5cd4-4056-9718-c9e9b6e11f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50264, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Placing the model to CUDA\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating answer"
      ],
      "metadata": {
        "id": "xf4fLTPdzwld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "1gfJFmeOBj_t",
        "outputId": "47f86bfa-1139-4512-87ce-bc8384731f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "prompts = ['Пошел мужик в лес', 'В газете написали, что', 'Васька сказал сегодня', 'Посмотри на это', 'Кто же мог подумать'] \n",
        "rows = []\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    inpt = tok.encode(prompt, return_tensors='pt')\n",
        "    out_07 = model.generate(inpt.cuda(), \n",
        "                        max_length=50, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=0.7)\n",
        "    \n",
        "    out_1 = model.generate(inpt.cuda(), \n",
        "                        max_length=50, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=1)\n",
        "    \n",
        "    out_13 = model.generate(inpt.cuda(), \n",
        "                        max_length=50, \n",
        "                        repetition_penalty=1.0, \n",
        "                        do_sample=True, \n",
        "                        top_k=20, \n",
        "                        top_p=0.92,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=1.3)\n",
        "\n",
        "    rows.append([prompt, \n",
        "             [tok.decode(example).replace('<s>', '').replace('</s>', '') for example in out_07],\n",
        "             [tok.decode(example).replace('<s>', '').replace('</s>', '') for example in out_1],\n",
        "             [tok.decode(example).replace('<s>', '').replace('</s>', '') for example in out_13]])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['Prompt', 'temp=0.7', 'temp=1.0', 'temp=1.3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "3XTm0hX1z4Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GA-erwbvDy5N",
        "outputId": "584d59dc-00b0-4658-d76c-0545e523a430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Prompt                                           temp=0.7  \\\n",
              "0       Пошел мужик в лес  [Пошел мужик в лес, заблудился и заблудился. А...   \n",
              "1  В газете написали, что  [В газете написали, что ты в этом году не рабо...   \n",
              "2   Васька сказал сегодня  [Васька сказал сегодня: \"Я - царь зверей!\" - Д...   \n",
              "3         Посмотри на это  [Посмотри на это место! Ты что, сдурела?! Там ...   \n",
              "4     Кто же мог подумать  [Кто же мог подумать, что у меня такой прыщ?!\\...   \n",
              "\n",
              "                                            temp=1.0  \\\n",
              "0  [Пошел мужик в лес на медведя, а там медведь с...   \n",
              "1  [В газете написали, что в Москве есть такая де...   \n",
              "2  [Васька сказал сегодня, что я его не видела, п...   \n",
              "3  [Посмотри на это. Это же мой кот! Нет, это кот...   \n",
              "4  [Кто же мог подумать! Это же я, Вовочка! А кто...   \n",
              "\n",
              "                                            temp=1.3  \n",
              "0  [Пошел мужик в лес, дрова рубить...\\nСын полка...  \n",
              "1  [В газете написали, что у Путина сын, а что до...  \n",
              "2  [Васька сказал сегодня: «Мам, у тебя же завтра...  \n",
              "3  [Посмотри на это зеркало. Почему? Я так хорошо...  \n",
              "4  [Кто же мог подумать такое! Ведь они сами же и...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c79e4af9-3202-4e74-b3d2-fcb07d62f04c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prompt</th>\n",
              "      <th>temp=0.7</th>\n",
              "      <th>temp=1.0</th>\n",
              "      <th>temp=1.3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Пошел мужик в лес</td>\n",
              "      <td>[Пошел мужик в лес, заблудился и заблудился. А...</td>\n",
              "      <td>[Пошел мужик в лес на медведя, а там медведь с...</td>\n",
              "      <td>[Пошел мужик в лес, дрова рубить...\\nСын полка...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>В газете написали, что</td>\n",
              "      <td>[В газете написали, что ты в этом году не рабо...</td>\n",
              "      <td>[В газете написали, что в Москве есть такая де...</td>\n",
              "      <td>[В газете написали, что у Путина сын, а что до...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Васька сказал сегодня</td>\n",
              "      <td>[Васька сказал сегодня: \"Я - царь зверей!\" - Д...</td>\n",
              "      <td>[Васька сказал сегодня, что я его не видела, п...</td>\n",
              "      <td>[Васька сказал сегодня: «Мам, у тебя же завтра...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Посмотри на это</td>\n",
              "      <td>[Посмотри на это место! Ты что, сдурела?! Там ...</td>\n",
              "      <td>[Посмотри на это. Это же мой кот! Нет, это кот...</td>\n",
              "      <td>[Посмотри на это зеркало. Почему? Я так хорошо...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кто же мог подумать</td>\n",
              "      <td>[Кто же мог подумать, что у меня такой прыщ?!\\...</td>\n",
              "      <td>[Кто же мог подумать! Это же я, Вовочка! А кто...</td>\n",
              "      <td>[Кто же мог подумать такое! Ведь они сами же и...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c79e4af9-3202-4e74-b3d2-fcb07d62f04c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c79e4af9-3202-4e74-b3d2-fcb07d62f04c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c79e4af9-3202-4e74-b3d2-fcb07d62f04c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}